# ResNet：深度残差网络

当大家还在惊叹GoogLeNet用结构化的连接纳入了大量卷积层的时候，微软亚洲研究院的研究员已经在设计更深但结构更简单的网络[ResNet](https://arxiv.org/abs/1512.03385)。他们凭借这个网络在2015年的Imagenet竞赛中大获全胜。

ResNet有效的解决了深度卷积神经网络难训练的问题。这是因为在误差反传的过程中，梯度通常变得越来越小，从而权重的更新量也变小。这个导致远离损失函数的层训练缓慢，随着层数的增加这个现象更加明显。之前有两种常用方案来尝试解决这个问题：

1. 按层训练。先训练靠近数据的层，然后慢慢的增加后面的层。但效果不是特别好，而且比较麻烦。
2. 使用更宽的层（增加输出通道）而不是更深来增加模型复杂度。但更宽的模型经常不如更深的效果好。

ResNet通过增加跨层的连接来解决梯度逐层回传时变小的问题。虽然这个想法之前就提出过了，但ResNet真正的把效果做好了。

下图演示了一个跨层的连接。

![](../img/residual.svg)


最底下那层的输入不仅仅是输出给了中间层，而且其与中间层结果相加进入最上层。这样在梯度反传时，最上层梯度可以直接跳过中间层传到最下层，从而避免最下层梯度过小情况。

为什么叫做残差网络呢？我们可以将上面示意图里的结构拆成两个网络的和，一个一层，一个两层，最下面层是共享的。

![](../img/residual2.svg)

在训练过程中，左边的网络因为更简单所以更容易训练。这个小网络没有拟合到的部分，或者说残差，则被右边的网络抓取住。所以直观上来说，即使加深网络，跨层连接仍然可以使得底层网络可以充分的训练，从而不会让训练更难。

## Residual块

ResNet沿用了VGG的那种全用$3\times 3$卷积，但在卷积和池化层之间加入了批量归一层来加速训练。每次跨层连接跨过两层卷积。这里我们定义一个这样的残差块。注意到如果输入的通道数和输出不一样时（`same_shape=False`），我们使用一个额外的$1\times 1$卷积来做通道变化，同时使用`strides=2`来把长宽减半。

```{.python .input  n=1}
from mxnet.gluon import nn
from mxnet import nd

class Residual(nn.Block):
    def __init__(self, channels, same_shape=True, **kwargs):
        super(Residual, self).__init__(**kwargs)
        self.same_shape = same_shape
        strides = 1 if same_shape else 2
        self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,
                              strides=strides)# 保证了input和output输出维度一样，strides=1
        self.bn1 = nn.BatchNorm()
        self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)# input和output输出维度一样
        self.bn2 = nn.BatchNorm()
        if not same_shape:
            self.conv3 = nn.Conv2D(channels, kernel_size=1,
                                  strides=strides)

    def forward(self, x):
        out = nd.relu(self.bn1(self.conv1(x)))# 中间的夹层是两层
        out = self.bn2(self.conv2(out))
        if not self.same_shape:
            x = self.conv3(x)
        return nd.relu(out + x)
```

```{.python .input}
class Residual(nn.Block):
    def __init__(self, channels, same_shape=True, **kwargs):
        super(Residual, self).__init__(**kwargs)
        self.same_shape = same_shape
        strides = 1 if same_shape else 2
        self.conv1 = nn.COnv2D(channels, kernel_size = 3, padding=1, 
                              strides = strides)
        self.bn1 = nn.BatchNorm()
        self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm()
        if not same_shape:
            self.conv3 = nn.Conv2D(channels, kernel_size=3,padding=1)
        
    def forward(self,x):
        out = nd.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if not self.same_shape:
            x = self.conv3(x)
        return nd.relu(out + x)
        
```

输入输出通道相同：

```{.python .input  n=2}
blk = Residual(3)
blk.initialize()

x = nd.random.uniform(shape=(4, 3, 6, 6))
blk(x).shape
```

```{.json .output n=2}
[
 {
  "data": {
   "text/plain": "(4, 3, 6, 6)"
  },
  "execution_count": 2,
  "metadata": {},
  "output_type": "execute_result"
 }
]
```

```{.python .input  n=4}
x
```

```{.json .output n=4}
[
 {
  "data": {
   "text/plain": "\n[[[[5.48813522e-01 5.92844605e-01 7.15189338e-01 8.44265759e-01\n    6.02763355e-01 8.57945621e-01]\n   [5.44883192e-01 8.47251713e-01 4.23654795e-01 6.23563707e-01\n    6.45894110e-01 3.84381711e-01]\n   [4.37587202e-01 2.97534615e-01 8.91772985e-01 5.67129776e-02\n    9.63662744e-01 2.72656292e-01]\n   [3.83441508e-01 4.77665126e-01 7.91725039e-01 8.12168717e-01\n    5.28894901e-01 4.79977161e-01]\n   [5.68044543e-01 3.92784804e-01 9.25596654e-01 8.36078763e-01\n    7.10360557e-02 3.37396175e-01]\n   [8.71292949e-02 6.48171902e-01 2.02183984e-02 3.68241549e-01\n    8.32619846e-01 9.57155168e-01]]\n\n  [[7.78156757e-01 1.40350774e-01 8.70012164e-01 8.70087266e-01\n    9.78618324e-01 4.73608047e-01]\n   [7.99158573e-01 8.00910771e-01 4.61479366e-01 5.20477474e-01\n    7.80529201e-01 6.78879559e-01]\n   [1.18274420e-01 7.20632672e-01 6.39921010e-01 5.82019806e-01\n    1.43353283e-01 5.37373245e-01]\n   [9.44668889e-01 7.58615613e-01 5.21848321e-01 1.05907604e-01\n    4.14661944e-01 4.73600417e-01]\n   [2.64555603e-01 1.86332345e-01 4.17021990e-01 9.97184813e-01\n    7.20324516e-01 9.32557344e-01]\n   [1.14381080e-04 1.28124446e-01 3.02332580e-01 9.99040544e-01\n    1.46755889e-01 2.36088976e-01]]\n\n  [[9.23385918e-02 3.96580726e-01 1.86260208e-01 3.87910753e-01\n    3.45560730e-01 6.69746041e-01]\n   [3.96767467e-01 9.35539067e-01 5.38816750e-01 8.46310914e-01\n    4.19194520e-01 3.13273519e-01]\n   [6.85219526e-01 5.24548173e-01 2.04452246e-01 4.43452895e-01\n    8.78117442e-01 2.29577214e-01]\n   [2.73875967e-02 5.34413934e-01 6.70467496e-01 9.13962007e-01\n    4.17304814e-01 4.57204819e-01]\n   [5.58689833e-01 4.30698574e-01 1.40386939e-01 9.39127803e-01\n    1.98101491e-01 7.78389215e-01]\n   [8.00744593e-01 7.15970516e-01 9.68261600e-01 8.02757502e-01\n    3.13424170e-01 9.28008109e-02]]]\n\n\n [[[6.92322612e-01 5.18152535e-01 8.76389146e-01 8.65020275e-01\n    8.94606650e-01 8.29146922e-01]\n   [8.50442052e-02 8.29603374e-01 3.90547849e-02 2.73049980e-01\n    1.69830427e-01 5.92431985e-02]\n   [8.78142476e-01 6.70528054e-01 9.83468369e-02 5.93065500e-01\n    4.35994893e-01 1.85082078e-01]\n   [2.59262286e-02 9.31540847e-01 5.49662471e-01 9.47730601e-01\n    4.35322404e-01 4.84749109e-01]\n   [4.20367807e-01 3.20536435e-01 3.30334812e-01 1.54426679e-01\n    2.04648629e-01 6.98862672e-01]\n   [6.19270980e-01 1.19950540e-01 2.99654663e-01 4.85175908e-01\n    2.66827285e-01 6.32737756e-01]]\n\n  [[6.21133804e-01 8.18226695e-01 5.29142082e-01 6.83026016e-01\n    1.34579942e-01 4.98561174e-01]\n   [5.13578117e-01 5.86796999e-01 1.84439868e-01 7.19754219e-01\n    7.85335124e-01 2.58498073e-01]\n   [8.53975296e-01 5.46207309e-01 4.94236827e-01 4.07307833e-01\n    8.46561491e-01 1.76984623e-01]\n   [7.96454698e-02 9.69632387e-01 5.05246103e-01 2.97018349e-01\n    6.52865022e-02 2.87868828e-01]\n   [4.28122342e-01 1.16193324e-01 9.65309218e-02 1.81727037e-01\n    1.27159968e-01 4.94289756e-01]\n   [5.96745312e-01 5.65765142e-01 2.26012006e-01 2.21835166e-01\n    1.06945686e-01 7.67491162e-01]]\n\n  [[2.20306203e-01 5.77308059e-01 3.49826276e-01 1.67823315e-01\n    4.67787474e-01 3.67471367e-01]\n   [5.50797880e-01 7.07248822e-02 7.08147824e-01 8.39949071e-01\n    2.90904731e-01 1.21328577e-01]\n   [5.10827601e-01 5.69311321e-01 8.92946959e-01 4.37061936e-01\n    8.96293104e-01 1.87480096e-02]\n   [1.25585318e-01 4.06307392e-02 2.07242876e-01 2.47888297e-01\n    5.14672063e-02 9.35514942e-02]\n   [4.40809846e-01 6.94823682e-01 2.98762135e-02 1.45370141e-01\n    4.56833214e-01 4.53172296e-01]\n   [6.49144053e-01 2.15577006e-01 2.78487295e-01 3.53905082e-01\n    6.76254928e-01 4.92635936e-01]]]\n\n\n [[[5.90862811e-01 9.13301468e-01 2.39818841e-02 7.65826404e-01\n    5.58854103e-01 9.73648250e-01]\n   [2.59252459e-01 4.02360648e-01 4.15101200e-01 5.52771926e-01\n    2.83525079e-01 3.06360632e-01]\n   [6.93137944e-01 5.86619973e-01 4.40453708e-01 2.74920106e-01\n    1.56867743e-01 4.54607755e-01]\n   [5.44649005e-01 7.57665217e-01 7.80314744e-01 8.17570746e-01\n    3.06363523e-01 2.57585287e-01]\n   [2.21957877e-01 8.93065095e-01 3.87971252e-01 7.18421996e-01\n    9.36383665e-01 2.06145011e-02]\n   [9.75995421e-01 1.42717332e-01 9.67029810e-01 9.00621474e-01\n    5.47232270e-01 1.72695324e-01]]\n\n  [[9.72684383e-01 8.55620921e-01 7.14815974e-01 6.09035611e-01\n    6.97728813e-01 5.97556233e-01]\n   [2.16089502e-01 1.41464174e-01 9.76274431e-01 2.24505931e-01\n    6.23025699e-03 6.98420227e-01]\n   [2.52982348e-01 9.03178513e-01 4.34791535e-01 6.32963121e-01\n    7.79382944e-01 5.15914988e-03]\n   [1.97685078e-01 5.72356462e-01 8.62993240e-01 3.61496329e-01\n    9.83400702e-01 6.04131937e-01]\n   [1.63842246e-01 3.91766191e-01 5.97333968e-01 8.11947763e-01\n    8.98609497e-03 6.12525344e-01]\n   [3.86571288e-01 2.66473770e-01 4.41600531e-02 6.43914282e-01\n    9.56652939e-01 8.11062098e-01]]\n\n  [[4.36146647e-01 1.90247409e-02 9.48977292e-01 3.97803634e-02\n    7.86305964e-01 9.45003867e-01]\n   [8.66289318e-01 4.46349204e-01 1.73165426e-01 4.41348523e-01\n    7.49485865e-02 6.57095388e-02]\n   [6.00742698e-01 1.75861239e-01 1.67972177e-01 8.65882754e-01\n    7.33380139e-01 8.43528092e-01]\n   [4.08443868e-01 9.25153136e-01 5.27908802e-01 8.70306492e-01\n    2.21993178e-01 5.51801212e-02]\n   [8.70732307e-01 8.31327856e-01 2.06719160e-01 3.63736898e-01\n    9.18610930e-01 9.79444981e-01]\n   [4.88411188e-01 8.98210332e-02 6.11743867e-01 3.96736592e-01\n    7.65907884e-01 3.54138047e-01]]]\n\n\n [[[5.18418014e-01 4.86638010e-01 2.96800494e-01 9.90820944e-01\n    1.87721223e-01 8.08281660e-01]\n   [8.07412714e-02 6.49458885e-01 7.38440275e-01 8.19472790e-01\n    4.41309214e-01 2.42540404e-01]\n   [1.58309862e-01 7.64372587e-01 8.79937053e-01 1.10900767e-01\n    2.74086475e-01 2.04154745e-01]\n   [4.14235026e-01 1.19095363e-01 2.96079934e-01 8.77903044e-01\n    6.28787935e-01 5.23675263e-01]\n   [5.79837799e-01 4.92136002e-01 5.99929214e-01 7.31871128e-01\n    2.65819132e-01 1.45807508e-02]\n   [2.84685880e-01 9.33630317e-02 2.53588200e-01 8.26554239e-01\n    3.27563941e-01 8.33492756e-01]]\n\n  [[1.44164294e-01 8.92410994e-01 1.65612862e-01 9.58013475e-01\n    9.63930547e-01 5.61337113e-01]\n   [9.60226715e-01 9.05486941e-02 1.88414648e-01 9.95988905e-01\n    2.43065599e-02 4.77401733e-01]\n   [8.92860174e-01 9.47476089e-01 3.31979811e-01 2.09406361e-01\n    8.21229100e-01 6.42546043e-02]\n   [4.16966267e-02 3.70109409e-01 1.07656673e-01 9.85028803e-01\n    5.95052063e-01 9.94716883e-02]\n   [5.29817343e-01 6.08473599e-01 4.18807417e-01 5.30396461e-01\n    3.35407853e-01 4.82832700e-01]\n   [6.22519433e-01 8.25209260e-01 4.38141435e-01 7.55012989e-01\n    7.35882103e-01 3.89975578e-01]]\n\n  [[5.18036425e-01 2.39813790e-01 5.78858614e-01 1.55615091e-01\n    6.45355105e-01 7.24328756e-01]\n   [9.90224242e-01 9.14195418e-01 8.19858193e-01 4.73026156e-01\n    4.13200945e-01 6.78503871e-01]\n   [8.76267672e-01 8.01720917e-01 8.23759437e-01 4.58184570e-01\n    5.44745028e-02 6.73834682e-01]\n   [7.18637228e-01 1.47844687e-01 8.02170575e-01 3.57095271e-01\n    7.36406624e-01 6.16476595e-01]\n   [7.09131777e-01 4.42510515e-01 5.40936828e-01 5.75952351e-01\n    1.24824174e-01 8.48412663e-02]\n   [9.57647324e-01 2.23283604e-01 4.03256297e-01 2.17890218e-01\n    2.16951162e-01 8.25591981e-01]]]]\n<NDArray 4x3x6x6 @cpu(0)>"
  },
  "execution_count": 4,
  "metadata": {},
  "output_type": "execute_result"
 }
]
```

输入输出通道不同：

```{.python .input  n=3}
blk2 = Residual(8, same_shape=False)
blk2.initialize()
blk2(x).shape
```

```{.json .output n=3}
[
 {
  "data": {
   "text/plain": "(4, 8, 3, 3)"
  },
  "execution_count": 3,
  "metadata": {},
  "output_type": "execute_result"
 }
]
```

## 构建ResNet

类似GoogLeNet主体是由Inception块串联而成，ResNet的主体部分串联多个Residual块。下面我们定义18层的ResNet。同样为了阅读更加容易，我们这里使用了多个`nn.Sequential`。另外注意到一点是，这里我们没用池化层来减小数据长宽，而是通过有通道变化的Residual块里面的使用`strides=2`的卷积层。

```{.python .input  n=5}
class ResNet(nn.Block):
    def __init__(self, num_classes, verbose=False, **kwargs):
        super(ResNet, self).__init__(**kwargs)
        self.verbose = verbose
        # add name_scope on the outermost Sequential
        with self.name_scope():
            # block 1
            b1 = nn.Conv2D(64, kernel_size=7, strides=2)
            # block 2
            b2 = nn.Sequential()
            b2.add(
                nn.MaxPool2D(pool_size=3, strides=2),
                Residual(64),
                Residual(64)
            )
            # block 3
            b3 = nn.Sequential()
            b3.add(
                Residual(128, same_shape=False),
                Residual(128)
            )
            # block 4
            b4 = nn.Sequential()
            b4.add(
                Residual(256, same_shape=False),
                Residual(256)
            )
            # block 5
            b5 = nn.Sequential()
            b5.add(
                Residual(512, same_shape=False),
                Residual(512)
            )
            # block 6
            b6 = nn.Sequential()
            b6.add(
                nn.AvgPool2D(pool_size=3),
                nn.Dense(num_classes)
            )
            # chain all blocks together
            self.net = nn.Sequential()
            self.net.add(b1, b2, b3, b4, b5, b6)

    def forward(self, x):
        out = x
        for i, b in enumerate(self.net):
            out = b(out)
            if self.verbose:
                print('Block %d output: %s'%(i+1, out.shape))
        return out
```

这里演示数据在块之间的形状变化：

```{.python .input  n=6}
net = ResNet(10, verbose=True)
net.initialize()

x = nd.random.uniform(shape=(4, 3, 96, 96))
y = net(x)
```

```{.json .output n=6}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Block 1 output: (4, 64, 45, 45)\nBlock 2 output: (4, 64, 22, 22)\nBlock 3 output: (4, 128, 11, 11)\nBlock 4 output: (4, 256, 6, 6)\nBlock 5 output: (4, 512, 3, 3)\nBlock 6 output: (4, 10)\n"
 }
]
```

## 获取数据并训练

跟前面类似，但因为有批量归一化，所以使用了较大的学习率。

```{.python .input}
import sys
sys.path.append('..')
import utils
from mxnet import gluon
from mxnet import init

train_data, test_data = utils.load_data_fashion_mnist(
    batch_size=64, resize=96)

ctx = utils.try_gpu()
net = ResNet(10)
net.initialize(ctx=ctx, init=init.Xavier())

loss = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(),
                        'sgd', {'learning_rate': 0.05})
utils.train(train_data, test_data, net, loss,
            trainer, ctx, num_epochs=1)
```

## 结论

ResNet使用跨层通道使得训练非常深的卷积神经网络成为可能。同样它使用很简单的卷积层配置，使得其拓展更加简单。

## 练习

- 这里我们实现了ResNet 18，原论文中还讨论了更深的配置。尝试实现它们。（提示：参考论文中的表1）
- 原论文中还介绍了一个“bottleneck”架构，尝试实现它
- ResNet作者在[接下来的一篇论文](https://arxiv.org/abs/1603.05027)讨论了将Residual块里面的`Conv->BN->Relu`结构改成了`BN->Relu->Conv`（参考论文图1），尝试实现它


**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1663)
