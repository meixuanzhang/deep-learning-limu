{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 `nn.Block` 来定义\n",
    "\n",
    "事实上，`nn.Sequential`是`nn.Block`的简单形式。我们先来看下如何使用`nn.Block`来实现同样的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到`nn.Block`的使用是通过创建一个它子类的类，其中至少包含了两个函数。\n",
    "\n",
    "- `__init__`：创建参数。上面例子我们使用了包含了参数的`dense`层\n",
    "- `forward()`：定义网络的计算\n",
    "\n",
    "我们所创建的类的使用跟前面`net`没有太多不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何定义创建和使用`nn.Dense`比较好理解。接下来我们仔细看下`MLP`里面用的其他命令：\n",
    "\n",
    "- `super(MLP, self).__init__(**kwargs)`：这句话调用`nn.Block`的`__init__`函数，它提供了`prefix`（指定名字）和`params`（指定模型参数）两个参数。我们会之后详细解释如何使用。\n",
    "\n",
    "- `self.name_scope()`：调用`nn.Block`提供的`name_scope()`函数。`nn.Dense`的定义放在这个`scope`里面。它的作用是给里面的所有层和参数的名字加上前缀（prefix）使得他们在系统里面独一无二。默认自动会自动生成前缀，我们也可以在创建的时候手动指定。推荐在构建网络时，每个层至少在一个`name_scope()`里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            #它的作用是给里面的所有层和参数的名字加上前缀（prefix）使得他们在系统里面独一无二。\n",
    "            self.dense0 = nn.Dense(256)\n",
    "            self.dense1 = nn.Dense(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense1(nd.relu(self.dense0(x)))\n",
    "    #x是输入数据，self.dense0(x)是输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = MLP()\n",
    "print(net2)\n",
    "#参数初始化\n",
    "net2.initialize()\n",
    "#输入样本X\n",
    "x = nd.random.uniform(shape=(4,20))\n",
    "#输出y\n",
    "y = net2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Block`到底是什么东西？\n",
    "\n",
    "在`gluon`里，`nn.Block`是一个一般化的部件。整个神经网络可以是一个`nn.Block`，单个层也是一个`nn.Block`。我们可以（近似）无限地嵌套`nn.Block`来构建新的`nn.Block`。\n",
    "\n",
    "`nn.Block`主要提供这个东西\n",
    "\n",
    "1. 存储参数\n",
    "2. 描述`forward`如何执行\n",
    "3. 自动求导\n",
    "## 那么现在可以解释`nn.Sequential`了吧\n",
    "\n",
    "`nn.Sequential`是一个`nn.Block`容器，它通过`add`来添加`nn.Block`。它自动生成`forward()`函数，其就是把加进来的`nn.Block`逐一运行。\n",
    "\n",
    "一个简单的实现是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sequential, self).__init__(**kwargs)\n",
    "    def add(self, block):\n",
    "        self._children.append(block)\n",
    "    def forward(self, x):\n",
    "        for block in self._children:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    #它的作用是给里面的所有层和参数的名字加上前缀（prefix）使得他们在系统里面独一无二。\n",
    "    net.add(nn.Dense(256, activation=\"relu\")) \n",
    "    net.add(nn.Dense(10))\n",
    "    \n",
    "net4.initialize()\n",
    "y = net4(x)##实际调用是net4.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Block):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense = nn.Dense(256)\n",
    "            #定义初始化权重\n",
    "            self.weight = nd.random_uniform(shape=(256,20))\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x=nd.relu(self.dense(x))\n",
    "        x=nd.relu(nd.dot(x,self.weight)+1)\n",
    "        x=nd.relu(self.dense(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "fancy_mlp = FancyMLP()\n",
    "fancy_mlp.initialize()\n",
    "y = fancy_mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): RecMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Dense(None -> 256, Activation(relu))\n",
      "      (1): Dense(None -> 128, Activation(relu))\n",
      "    )\n",
      "    (dense): Dense(None -> 64, linear)\n",
      "  )\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RecMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RecMLP, self).__init__(**kwargs)\n",
    "        #`nn.Block`和`nn.Sequential`的嵌套使用\n",
    "        self.net = nn.Sequential()\n",
    "        with self.name_scope():\n",
    "            self.net.add(nn.Dense(256, activation=\"relu\"))\n",
    "            self.net.add(nn.Dense(128, activation=\"relu\"))\n",
    "            self.dense = nn.Dense(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nd.relu(self.dense(self.net(x)))\n",
    "\n",
    "rec_mlp = nn.Sequential()\n",
    "rec_mlp.add(RecMLP())\n",
    "rec_mlp.add(nn.Dense(10))\n",
    "print(rec_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化模型参数&访问模型参数\n",
    "\n",
    "之前我们提到过可以通过`weight`和`bias`访问`Dense`的参数，他们是`Parameter`这个类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 3.7522419e-05  1.3728756e-03]\n",
       " [-2.9933348e-03  2.0859984e-03]\n",
       " [-2.1478857e-03 -1.9505927e-03]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(4, activation=\"relu\"))\n",
    "        net.add(nn.Dense(2))\n",
    "    return net\n",
    "\n",
    "x = nd.random.uniform(shape=(3,5))\n",
    "net = get_net()\n",
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  sequential2_dense0 \n",
      "weight:  Parameter sequential2_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>) \n",
      "bias:  Parameter sequential2_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight\n",
    "b = net[0].bias\n",
    "print('name: ', net[0].name, '\\nweight: ', w, '\\nbias: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: \n",
      "[[-0.0578019   0.02074406 -0.06716943 -0.01844618  0.04656678]\n",
      " [ 0.06400172  0.03894195 -0.05035089  0.0518017   0.05181222]\n",
      " [ 0.06700657 -0.00369488  0.0418822   0.0421275  -0.00539289]\n",
      " [ 0.00286685  0.03927409  0.02504314 -0.05344158  0.03088857]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "weight gradient \n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "bias: \n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "bias gradient \n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('weight:', w.data())\n",
    "print('weight gradient', w.grad())\n",
    "print('bias:', b.data())\n",
    "print('bias gradient', b.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential2_ (\n",
      "  Parameter sequential2_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential2_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential2_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential2_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[[-0.0578019   0.02074406 -0.06716943 -0.01844618  0.04656678]\n",
      " [ 0.06400172  0.03894195 -0.05035089  0.0518017   0.05181222]\n",
      " [ 0.06700657 -0.00369488  0.0418822   0.0421275  -0.00539289]\n",
      " [ 0.00286685  0.03927409  0.02504314 -0.05344158  0.03088857]]\n",
      "<NDArray 4x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params() #返回的是class mxnet.gluon.ParameterDict \n",
    "print(params)\n",
    "print(params['sequential2_dense0_bias'].data())\n",
    "print(params.get('dense0_weight').data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用不同的初始函数来初始化\n",
    "我们一直在使用默认的`initialize`来初始化权重（除了指定GPU `ctx`外）。它会把所有权重初始化成在`[-0.07, 0.07]`之间均匀分布的随机数。我们可以使用别的初始化方法。例如使用均值为0，方差为0.02的正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.01551393 -0.01576435  0.01483546 -0.02946888 -0.02146186]\n",
      " [-0.02084965 -0.0265577  -0.02949932 -0.01048284  0.02532511]\n",
      " [ 0.01790128 -0.01203189  0.02408112 -0.01942439 -0.01165124]\n",
      " [ 0.00743415  0.01860014 -0.02845151 -0.0103524   0.04017665]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import init\n",
    "params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)#Pclass mxnet.gluon.Parameter   (class mxnet.initializer.Initializer)init\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params.initialize(init=init.One(), force_reinit=True)\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 8.616555  8.616555]\n",
       " [12.723242 12.723242]\n",
       " [10.25157  10.25157 ]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共享模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(4, activation=\"relu\"))\n",
    "    net.add(nn.Dense(4, activation=\"relu\"))\n",
    "    net.add(nn.Dense(4, activation=\"relu\", params=net[-1].params))#当前net[-1]是上一层\n",
    "    net.add(nn.Dense(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.02188614 -0.02559176 -0.05065439  0.03896836 -0.04247847]\n",
      " [ 0.06293995 -0.01837847  0.02275376  0.04493906 -0.06809997]\n",
      " [-0.05640582  0.01719845  0.04731229  0.02431235 -0.05654623]\n",
      " [ 0.06607229  0.06670433  0.05294709 -0.00438883  0.00134741]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "\n",
      "[[ 0.06674656 -0.06219994  0.01467837 -0.00683771]\n",
      " [ 0.0334969  -0.06720173 -0.06451371 -0.00816047]\n",
      " [-0.03040703  0.06714214 -0.05317248 -0.01967777]\n",
      " [-0.02854037 -0.00267491 -0.05337812  0.02641256]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[ 0.06674656 -0.06219994  0.01467837 -0.00683771]\n",
      " [ 0.0334969  -0.06720173 -0.06451371 -0.00816047]\n",
      " [-0.03040703  0.06714214 -0.05317248 -0.01967777]\n",
      " [-0.02854037 -0.00267491 -0.05337812  0.02641256]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[-0.02548236  0.05326662 -0.01200318  0.05855297]\n",
      " [-0.06101935 -0.0396449   0.0269461   0.00912645]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net.initialize() #默认weight初始化是均匀分布继承了block的initialize？？\n",
    "net(x)\n",
    "print(net[0].weight.data())\n",
    "print(net[1].weight.data())\n",
    "print(net[2].weight.data())\n",
    "print(net[3].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义初始化方法\n",
    "下面我们自定义一个初始化方法。它通过重载`_init_weight`来实现不同的初始化方法。（注意到Gluon里面`bias`都是默认初始化成0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInit(init.Initializer):#class mxnet.initializer.Initializer??重载 init=..也会重载??\n",
    "    def __init__(self):\n",
    "        super(MyInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        # 初始化权重，使用out=arr后我们不需指定形状\n",
    "        #print('init weight', arr.shape)\n",
    "        nd.random.uniform(low=5, high=10, out=arr)\n",
    "    def _init_bias(self, _, arr):\n",
    "        #print('init_bias',arr.shape)\n",
    "        arr[:] = 1.0\n",
    "        #print('init_bias',arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_net()\n",
    "net.initialize(MyInit())\n",
    "net(x)\n",
    "net[0].bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、self._verbose = True是实现什么功能？\n",
    "2、“ 初始化权重，使用out=arr后我们不需指定形状”这个没整明白，这段代码是实现初始化权重参数的原理是啥呢？\n",
    "3、为啥bias没有被初始化呢？（注释中的“FIXME”难道表示这个是一个没修的bug吗？）\n",
    "\n",
    "\n",
    "第三点应该是bug，应该是gluon里面的parameter.py实现中忽略了bias\n",
    "可以采用一种蛮无聊的强制初始化参数：\n",
    "net[0].bias.initialize(init=MyInit(), force_reinit=True) （此时bias也会被赋予5~10，因此如果采用这种方式需要再写一个MyInit类）\n",
    "\n",
    "第一点是为了“输出更全的信息”：这在后面教程中比如 y=net(x)时会输出每层的输出信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "当然我们也可以通过`Parameter.set_data`来直接改写权重。注意到由于有延后初始化，所以我们通常可以通过调用一次`net(x)`来确定权重的形状先。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_net()\n",
    "net.initialize()\n",
    "net(x)\n",
    "\n",
    "print('default weight:', net[1].weight.data())\n",
    "\n",
    "w = net[1].weight\n",
    "w.set_data(nd.ones(w.shape))\n",
    "\n",
    "print('init to all 1s:', net[1].weight.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
